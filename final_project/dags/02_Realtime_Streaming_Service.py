"""
Realtime Streaming Prediction Pipeline - Improved Architecture
- T√°ch bi·ªát ho√†n to√†n v·ªõi Training DAG
- T·ª± ƒë·ªông ki·ªÉm tra v√† load model m·ªõi nh·∫•t
- Graceful shutdown cho streaming job
- Health check v√† monitoring
"""
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.empty import EmptyOperator  # Changed from dummy to empty
from datetime import datetime, timedelta
import time
from mycelery.system_worker import run_command, docker_compose_down, docker_compose_up

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

# Configuration
HDFS_NAMENODE = "hdfs://192.168.80.178:8020"
SPARK_MASTER = "spark://192.168.80.165:7077"
KAFKA_BROKER = "192.168.80.51:9092"
KAFKA_QUEUE = "nole2"
SPARK_QUEUE = "nole1"
HADOOP_QUEUE = "nole3"

# Model configuration
MODEL_BASE_PATH = "/bigdata/house_prices/models"  # Thay ƒë·ªïi ƒë·ªÉ l∆∞u nhi·ªÅu versions
CURRENT_MODEL_LINK = "/bigdata/house_prices/model_current"  # Symbolic link to latest model

def wait_for_task(result, timeout=300):
    """Wait for Celery task to complete"""
    elapsed = 0
    while elapsed < timeout:
        if result.ready():
            if result.successful():
                return result.result
            else:
                raise Exception(f"Task failed: {result.info}")
        time.sleep(2)
        elapsed += 2
    raise TimeoutError(f"Task timeout after {timeout}s")

with DAG(
    '02_Realtime_Streaming_Service',
    default_args=default_args,
    description='Realtime Streaming Prediction Pipeline (Improved)',
    schedule=None,  # Manual trigger ho·∫∑c schedule theo nhu c·∫ßu
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['streaming', 'prediction', 'production'],
) as dag:

    # ========================================
    # PHASE 0: PRE-CHECK - Model Availability
    # ========================================
    
    def check_and_get_model(**context):
        """
        Ki·ªÉm tra model tr√™n HDFS, n·∫øu kh√¥ng c√≥ th√¨ t√¨m t·ª´ model base path
        Return: 'continue_setup' n·∫øu t√¨m th·∫•y model, raise Exception n·∫øu kh√¥ng
        """
        print("üîç Step 1: Checking if model exists at current link...")
        
        # Try to find model at current link first
        try:
            result = run_command.apply_async(
                args=[f"~/hadoop/bin/hdfs dfs -ls {CURRENT_MODEL_LINK}/"],
                queue=HADOOP_QUEUE
            )
            output = wait_for_task(result, timeout=30)
            
            # Handle dict response
            if isinstance(output, dict):
                output = output.get('stdout', '') or output.get('output', '')
            
            output_str = str(output) if output else ""
            
            if "metadata" in output_str.lower() or "data" in output_str.lower():
                print(f"‚úÖ Model found at {CURRENT_MODEL_LINK}")
                print(f"Model contents:\n{output_str}")
                context['task_instance'].xcom_push(key='model_path', value=CURRENT_MODEL_LINK)
                return 'continue_setup'
        except Exception as e:
            print(f"‚ö†Ô∏è  No model at current link: {str(e)}")
        
        # If not found, try to find latest version
        print("üîç Step 2: Searching for latest model version in base path...")
        try:
            result = run_command.apply_async(
                args=[f"~/hadoop/bin/hdfs dfs -ls {MODEL_BASE_PATH}/ | grep version_ | tail -1"],
                queue=HADOOP_QUEUE
            )
            output = wait_for_task(result, timeout=30)
            
            # Handle dict response
            if isinstance(output, dict):
                output = output.get('stdout', '') or output.get('output', '')
            
            output_str = str(output) if output else ""
            
            if output_str and "version_" in output_str:
                # Extract model path from ls output
                model_path = output_str.strip().split()[-1]
                print(f"‚úÖ Found latest model: {model_path}")
                
                # Update symbolic link to point to latest model
                print(f"üîó Creating link from {CURRENT_MODEL_LINK} to {model_path}")
                update_link_cmd = f"""
                ~/hadoop/bin/hdfs dfs -rm -r {CURRENT_MODEL_LINK} 2>/dev/null || true && \
                ~/hadoop/bin/hdfs dfs -cp {model_path} {CURRENT_MODEL_LINK}
                """
                result = run_command.apply_async(args=[update_link_cmd], queue=HADOOP_QUEUE)
                wait_for_task(result, timeout=30)
                
                context['task_instance'].xcom_push(key='model_path', value=CURRENT_MODEL_LINK)
                print(f"‚úÖ Model link created successfully")
                return 'continue_setup'
        except Exception as e:
            print(f"‚ùå Error finding latest model: {str(e)}")
        
        # If still not found, check for legacy model path
        print("üîç Step 3: Checking legacy model path...")
        try:
            legacy_path = "/bigdata/house_prices/model"
            result = run_command.apply_async(
                args=[f"~/hadoop/bin/hdfs dfs -ls {legacy_path}/"],
                queue=HADOOP_QUEUE
            )
            output = wait_for_task(result, timeout=30)
            
            # Handle dict response
            if isinstance(output, dict):
                output = output.get('stdout', '') or output.get('output', '')
            
            output_str = str(output) if output else ""
            
            if "metadata" in output_str.lower() or "data" in output_str.lower():
                print(f"‚úÖ Found model at legacy path: {legacy_path}")
                context['task_instance'].xcom_push(key='model_path', value=legacy_path)
                return 'continue_setup'
        except Exception as e:
            print(f"‚ö†Ô∏è  No model at legacy path: {str(e)}")
        
        # No model found anywhere - fail the DAG
        print("=" * 60)
        print("‚ùå FATAL ERROR: NO TRAINED MODEL FOUND!")
        print("=" * 60)
        print("Searched locations:")
        print(f"  1. Current link: {CURRENT_MODEL_LINK}")
        print(f"  2. Model versions: {MODEL_BASE_PATH}/version_*")
        print(f"  3. Legacy path: /bigdata/house_prices/model")
        print("\n‚ö†Ô∏è  ACTION REQUIRED:")
        print("Please run the Training DAG (01_Train_Model) first to train a model.")
        print("=" * 60)
        raise Exception("No trained model available. Run training DAG first.")
    
    check_model = BranchPythonOperator(
        task_id='check_and_get_model',
        python_callable=check_and_get_model,
    )
    
    continue_setup = EmptyOperator(task_id='continue_setup')

    # ========================================
    # PHASE 1: KAFKA SETUP
    # ========================================
    
    def check_kafka_status(**context):
        """Check if Kafka is already running"""
        print("üîç Checking Kafka status...")
        try:
            result = run_command.apply_async(
                args=["docker ps | grep kafka"],
                queue=KAFKA_QUEUE
            )
            output = wait_for_task(result, timeout=30)
            
            # Handle dict response
            if isinstance(output, dict):
                output = output.get('stdout', '') or output.get('output', '')
            
            output_str = str(output) if output else ""
            
            if output_str and "kafka" in output_str.lower():
                print("‚úÖ Kafka is already running")
                return 'kafka_running'
            else:
                print("‚ö†Ô∏è  Kafka is not running")
                return 'kafka_not_running'
        except:
            print("‚ö†Ô∏è  Kafka is not running")
            return 'kafka_not_running'
    
    def stop_kafka_task(**context):
        print("üõë Stopping Kafka...")
        result = docker_compose_down.apply_async(
            args=["~/kafka_docker/docker-compose.yml"],
            queue=KAFKA_QUEUE
        )
        wait_for_task(result, timeout=60)
        print("‚úÖ Kafka stopped")

    def start_kafka_task(**context):
        print("üöÄ Starting Kafka...")
        result = docker_compose_up.apply_async(
            args=["~/kafka_docker/docker-compose.yml"],
            kwargs={"detach": True},
            queue=KAFKA_QUEUE
        )
        wait_for_task(result, timeout=120)
        time.sleep(15)  # Wait for Kafka to be fully ready
        print("‚úÖ Kafka started")
    
    def setup_kafka_topics(**context):
        """Create topics with proper configuration"""
        print("üìã Setting up Kafka topics...")
        topics = [
            {
                "name": "house_input",
                "partitions": 3,  # Increased for better parallelism
                "replication": 1,
                "config": "retention.ms=86400000"  # 24 hours retention
            },
            {
                "name": "house_prediction",
                "partitions": 3,
                "replication": 1,
                "config": "retention.ms=86400000"
            }
        ]
        
        for topic in topics:
            cmd = f"""docker exec kafka kafka-topics \
                --create --if-not-exists \
                --bootstrap-server localhost:9092 \
                --replication-factor {topic['replication']} \
                --partitions {topic['partitions']} \
                --topic {topic['name']} \
                --config {topic['config']}"""
            
            result = run_command.apply_async(args=[cmd], queue=KAFKA_QUEUE)
            wait_for_task(result, timeout=30)
            print(f"‚úÖ Topic '{topic['name']}' configured")
    
    def verify_kafka_health(**context):
        """Verify Kafka is healthy and topics exist"""
        print("üè• Verifying Kafka health...")
        
        # List topics
        result = run_command.apply_async(
            args=["docker exec kafka kafka-topics --list --bootstrap-server localhost:9092"],
            queue=KAFKA_QUEUE
        )
        topics = wait_for_task(result, timeout=30)
        
        # Handle dict response
        if isinstance(topics, dict):
            topics = topics.get('stdout', '') or topics.get('output', '')
        
        print(f"üìã Available topics:\n{topics}")
        
        # Check broker status
        result = run_command.apply_async(
            args=["docker exec kafka kafka-broker-api-versions --bootstrap-server localhost:9092 2>&1 | head -5"],
            queue=KAFKA_QUEUE
        )
        broker_info = wait_for_task(result, timeout=30)
        
        # Handle dict response
        if isinstance(broker_info, dict):
            broker_info = broker_info.get('stdout', '') or broker_info.get('output', '')
        
        print(f"üîß Broker status: OK")
        print("‚úÖ Kafka health check passed")
    
    check_kafka = BranchPythonOperator(
        task_id='check_kafka_status',
        python_callable=check_kafka_status,
    )
    
    kafka_running = EmptyOperator(task_id='kafka_running')
    kafka_not_running = EmptyOperator(task_id='kafka_not_running')
    
    stop_kafka = PythonOperator(
        task_id='stop_kafka',
        python_callable=stop_kafka_task,
    )
    
    start_kafka = PythonOperator(
        task_id='start_kafka',
        python_callable=start_kafka_task,
    )
    
    setup_topics = PythonOperator(
        task_id='setup_kafka_topics',
        python_callable=setup_kafka_topics,
        trigger_rule='none_failed',
    )
    
    verify_kafka = PythonOperator(
        task_id='verify_kafka_health',
        python_callable=verify_kafka_health,
    )

    # ========================================
    # PHASE 2: SPARK STREAMING JOB MANAGEMENT
    # ========================================
    
    def check_streaming_job_status(**context):
        """Check if streaming job is already running"""
        print("üîç Checking existing streaming jobs...")
        try:
            result = run_command.apply_async(
                args=["pgrep -f spark_streaming.py"],
                queue=SPARK_QUEUE
            )
            output = wait_for_task(result, timeout=30)
            
            # Handle dict response
            if isinstance(output, dict):
                output = output.get('stdout', '') or output.get('output', '')
            
            output_str = str(output).strip() if output else ""
            
            if output_str:
                print(f"‚ö†Ô∏è  Found existing streaming job(s): PID {output_str}")
                return 'streaming_running'
            else:
                print("‚úÖ No existing streaming jobs found")
                return 'streaming_not_running'
        except:
            print("‚úÖ No existing streaming jobs found")
            return 'streaming_not_running'
    
    def stop_streaming_job_gracefully(**context):
        """Gracefully stop existing streaming job"""
        print("üõë Stopping existing Spark Streaming jobs...")
        
        # Send SIGTERM for graceful shutdown
        result = run_command.apply_async(
            args=["pkill -15 -f spark_streaming.py || true"],
            queue=SPARK_QUEUE
        )
        wait_for_task(result, timeout=30)
        
        # Wait for graceful shutdown
        time.sleep(10)
        
        # Force kill if still running
        result = run_command.apply_async(
            args=["pkill -9 -f spark_streaming.py || true"],
            queue=SPARK_QUEUE
        )
        wait_for_task(result, timeout=30)
        
        print("‚úÖ Streaming jobs stopped")

    def start_streaming_job_task(**context):
        """Start Spark Streaming job with model path from XCom"""
        print("üöÄ Starting Spark Streaming job...")
        
        # Get model path from XCom
        model_path = context['task_instance'].xcom_pull(
            task_ids='check_and_get_model',
            key='model_path'
        )
        
        print(f"üì¶ Using model from: {model_path}")
        
        # Updated spark-submit with better configuration
        spark_submit_cmd = f"""
        nohup ~/spark/bin/spark-submit \
          --master {SPARK_MASTER} \
          --deploy-mode client \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
          --conf spark.hadoop.fs.defaultFS={HDFS_NAMENODE} \
          --conf spark.driver.memory=2g \
          --conf spark.executor.memory=2g \
          --conf spark.executor.cores=2 \
          --conf spark.cores.max=4 \
          --conf spark.streaming.kafka.maxRatePerPartition=100 \
          --conf spark.sql.streaming.checkpointLocation=/tmp/spark_checkpoint \
          ~/Tuan/Project/BigData/final_project/spark_code/spark_streaming.py \
          --model-path {model_path} \
          --kafka-broker {KAFKA_BROKER} \
          > /tmp/spark_streaming.log 2>&1 &
        """
        
        result = run_command.apply_async(
            args=[spark_submit_cmd],
            queue=SPARK_QUEUE
        )
        
        # Don't wait for completion - streaming job runs continuously
        print(f"‚úÖ Streaming job submitted. Task ID: {result.id}")
        print("üìä Check logs at: /tmp/spark_streaming.log")
        print(f"üåê Spark UI: http://{SPARK_MASTER.replace('spark://', '').split(':')[0]}:4040")
        
        # Wait a bit and verify job started
        time.sleep(15)
        
    def verify_streaming_job(**context):
        """Verify streaming job is running properly"""
        print("üè• Verifying streaming job health...")
        
        # Check if process is running
        result = run_command.apply_async(
            args=["pgrep -f spark_streaming.py"],
            queue=SPARK_QUEUE
        )
        
        try:
            output = wait_for_task(result, timeout=30)
            
            # Handle both dict and string responses
            if isinstance(output, dict):
                output = output.get('stdout', '') or output.get('output', '')
            
            output_str = str(output).strip() if output else ""
            
            if output_str:
                print(f"‚úÖ Streaming job is running (PID: {output_str})")
                
                # Check logs for errors
                result = run_command.apply_async(
                    args=["tail -50 /tmp/spark_streaming.log 2>/dev/null || echo 'Log file not found'"],
                    queue=SPARK_QUEUE
                )
                logs = wait_for_task(result, timeout=30)
                
                # Handle dict response for logs
                if isinstance(logs, dict):
                    logs = logs.get('stdout', '') or logs.get('output', '')
                
                logs_str = str(logs) if logs else ""
                
                if "error" in logs_str.lower() or "exception" in logs_str.lower():
                    print("‚ö†Ô∏è  Warning: Errors detected in logs")
                    print(f"Recent logs:\n{logs_str[:500]}")  # Limit log output
                else:
                    print("‚úÖ No errors in recent logs")
                    
                return True
            else:
                raise Exception("Streaming job process not found")
        except Exception as e:
            print(f"‚ùå Streaming job verification failed: {str(e)}")
            raise
    
    check_streaming = BranchPythonOperator(
        task_id='check_streaming_status',
        python_callable=check_streaming_job_status,
    )
    
    streaming_running = EmptyOperator(task_id='streaming_running')
    streaming_not_running = EmptyOperator(task_id='streaming_not_running')
    
    stop_streaming = PythonOperator(
        task_id='stop_streaming_gracefully',
        python_callable=stop_streaming_job_gracefully,
    )
    
    start_streaming = PythonOperator(
        task_id='start_streaming_job',
        python_callable=start_streaming_job_task,
        trigger_rule='none_failed',
    )
    
    verify_streaming = PythonOperator(
        task_id='verify_streaming_health',
        python_callable=verify_streaming_job,
    )

    # ========================================
    # PHASE 3: FINAL STATUS REPORT
    # ========================================
    
    def final_status_report(**context):
        """Generate final status report"""
        print("\n" + "=" * 80)
        print("üéâ STREAMING PREDICTION SERVICE - STATUS REPORT")
        print("=" * 80)
        
        model_path = context['task_instance'].xcom_pull(
            task_ids='check_and_get_model',
            key='model_path'
        )
        
        print(f"""
‚úÖ Service Status: RUNNING
üì¶ Model Path: {model_path}
üîß Kafka Broker: {KAFKA_BROKER}
‚ö° Spark Master: {SPARK_MASTER}

üìä Monitoring:
   - Spark UI: http://{SPARK_MASTER.replace('spark://', '').split(':')[0]}:4040
   - Logs: /tmp/spark_streaming.log
   - Kafka Topics: house_input, house_prediction

üìù Usage:
   1. Send input data to Kafka topic 'house_input'
   2. Predictions will be available in 'house_prediction' topic
   3. Monitor logs for any issues

‚ö†Ô∏è  To stop the service, run the Cleanup DAG or manually kill the process
        """)
        print("=" * 80 + "\n")
    
    status_report = PythonOperator(
        task_id='final_status_report',
        python_callable=final_status_report,
        trigger_rule='none_failed',
    )

    # ========================================
    # DEPENDENCY GRAPH
    # ========================================
    
    # Phase 0: Check model (single task, no cycle)
    check_model >> continue_setup
    
    # Phase 1: Setup Kafka
    continue_setup >> check_kafka >> [kafka_running, kafka_not_running]
    kafka_not_running >> stop_kafka >> start_kafka >> setup_topics
    kafka_running >> setup_topics
    setup_topics >> verify_kafka
    
    # Phase 2: Setup Streaming
    verify_kafka >> check_streaming >> [streaming_running, streaming_not_running]
    streaming_running >> stop_streaming
    streaming_not_running >> stop_streaming
    stop_streaming >> start_streaming >> verify_streaming
    
    # Phase 3: Final report
    verify_streaming >> status_report