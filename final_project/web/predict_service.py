"""
Service ƒë·ªÉ load Spark ML model v√† th·ª±c hi·ªán d·ª± ƒëo√°n gi√° nh√†
H·ªó tr·ª£ load t·ª´ HDFS ho·∫∑c local
"""
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, DoubleType
from pyspark.sql import Row
import os
import subprocess
import tempfile
import shutil

class HousePricePredictor:
    def __init__(self, model_path=None):
        # C√≥ th·ªÉ override b·∫±ng environment variable
        if model_path is None:
            model_path = os.getenv("MODEL_PATH", "models/house_price_model")
        
        # N·∫øu l√† HDFS path, s·∫Ω download v·ªÅ local t·∫°m
        self.model_path = model_path
        self.hdfs_model_path = None
        self.temp_model_dir = None
        self.spark = None
        self.model = None
        self._initialize()
    
    def _download_from_hdfs(self, hdfs_path):
        """Download model t·ª´ HDFS v·ªÅ local"""
        # T·∫°o th∆∞ m·ª•c t·∫°m
        temp_dir = tempfile.mkdtemp(prefix="spark_model_")
        self.temp_model_dir = temp_dir
        
        try:
            # Download to√†n b·ªô th∆∞ m·ª•c model t·ª´ HDFS
            cmd = ['hdfs', 'dfs', '-get', hdfs_path, temp_dir]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                raise Exception(
                    f"Kh√¥ng th·ªÉ download model t·ª´ HDFS: {hdfs_path}\n"
                    f"Error: {result.stderr}\n"
                    f"Command: {' '.join(cmd)}"
                )
            
            # T√¨m ƒë∆∞·ªùng d·∫´n model trong temp_dir
            # HDFS get s·∫Ω t·∫°o th∆∞ m·ª•c v·ªõi t√™n cu·ªëi c√πng c·ªßa path
            model_name = os.path.basename(hdfs_path.rstrip('/'))
            local_model_path = os.path.join(temp_dir, model_name)
            
            if not os.path.exists(local_model_path):
                # Th·ª≠ t√¨m trong temp_dir
                contents = os.listdir(temp_dir)
                if contents:
                    local_model_path = os.path.join(temp_dir, contents[0])
            
            print(f"‚úì ƒê√£ download model t·ª´ HDFS: {hdfs_path} ‚Üí {local_model_path}")
            return local_model_path
            
        except Exception as e:
            # Cleanup n·∫øu l·ªói
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)
            raise
    
    def _initialize(self):
        """Kh·ªüi t·∫°o Spark session v√† load model"""
        try:
            # Ki·ªÉm tra xem model_path c√≥ ph·∫£i HDFS path kh√¥ng
            is_hdfs = self.model_path.startswith('hdfs://')
            
            if is_hdfs:
                self.hdfs_model_path = self.model_path
                # Download t·ª´ HDFS v·ªÅ local
                local_model_path = self._download_from_hdfs(self.hdfs_model_path)
                # ƒê·∫£m b·∫£o d√πng absolute path v·ªõi file:// prefix
                local_model_path = os.path.abspath(local_model_path)
                if not local_model_path.startswith('file://'):
                    local_model_path = f"file://{local_model_path}"
            else:
                # Load t·ª´ local
                local_model_path = self.model_path
                if not os.path.exists(local_model_path):
                    raise FileNotFoundError(f"Model kh√¥ng t·ªìn t·∫°i t·∫°i: {local_model_path}")
                # ƒê·∫£m b·∫£o d√πng absolute path v·ªõi file:// prefix
                local_model_path = os.path.abspath(local_model_path)
                if not local_model_path.startswith('file://'):
                    local_model_path = f"file://{local_model_path}"
            
            # Spark session cho local filesystem (lu√¥n d√πng file:// khi load model)
            self.spark = SparkSession.builder \
                .appName("HousePricePredictionService") \
                .config("spark.hadoop.fs.defaultFS", "file:///") \
                .config("spark.local.dir", "/tmp/spark_local") \
                .config("spark.driver.memory", "2g") \
                .config("spark.executor.memory", "2g") \
                .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("ERROR")
            
            # Load model v·ªõi file:// path
            print(f"üìÇ ƒêang load model t·ª´: {local_model_path}")
            self.model = PipelineModel.load(local_model_path)
            print(f"‚úì ƒê√£ t·∫£i model th√†nh c√¥ng t·ª´: {self.model_path}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi kh·ªüi t·∫°o predictor: {e}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()}")
            raise
    
    def predict(self, med_inc, house_age, ave_rooms, ave_bedrms, 
                population, ave_occup, latitude, longitude):
        """
        D·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng
        
        Args:
            med_inc: Thu nh·∫≠p trung b√¨nh
            house_age: Tu·ªïi nh√†
            ave_rooms: S·ªë ph√≤ng trung b√¨nh
            ave_bedrms: S·ªë ph√≤ng ng·ªß trung b√¨nh
            population: D√¢n s·ªë
            ave_occup: M·∫≠t ƒë·ªô c∆∞ tr√∫ trung b√¨nh
            latitude: Vƒ© ƒë·ªô
            longitude: Kinh ƒë·ªô
        
        Returns:
            float: Gi√° nh√† d·ª± ƒëo√°n (ƒë∆°n v·ªã: trƒÉm ngh√¨n USD)
        """
        try:
            # T·∫°o DataFrame t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o
            data = Row(
                MedInc=float(med_inc),
                HouseAge=float(house_age),
                AveRooms=float(ave_rooms),
                AveBedrms=float(ave_bedrms),
                Population=float(population),
                AveOccup=float(ave_occup),
                Latitude=float(latitude),
                Longitude=float(longitude)
            )
            
            df = self.spark.createDataFrame([data])
            
            # Th·ª±c hi·ªán d·ª± ƒëo√°n
            predictions = self.model.transform(df)
            
            # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n
            result = predictions.select("prediction").collect()[0][0]
            
            return float(result)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi d·ª± ƒëo√°n: {e}")
            raise
    
    def predict_batch(self, data_list):
        """
        D·ª± ƒëo√°n h√†ng lo·∫°t
        
        Args:
            data_list: List of dicts, m·ªói dict ch·ª©a c√°c features
        
        Returns:
            List of predictions
        """
        try:
            rows = []
            for data in data_list:
                rows.append(Row(
                    MedInc=float(data['MedInc']),
                    HouseAge=float(data['HouseAge']),
                    AveRooms=float(data['AveRooms']),
                    AveBedrms=float(data['AveBedrms']),
                    Population=float(data['Population']),
                    AveOccup=float(data['AveOccup']),
                    Latitude=float(data['Latitude']),
                    Longitude=float(data['Longitude'])
                ))
            
            df = self.spark.createDataFrame(rows)
            predictions = self.model.transform(df)
            
            results = [float(row.prediction) for row in predictions.select("prediction").collect()]
            return results
            
        except Exception as e:
            print(f"‚ùå L·ªói khi d·ª± ƒëo√°n h√†ng lo·∫°t: {e}")
            raise
    
    def close(self):
        """ƒê√≥ng Spark session v√† cleanup"""
        if self.spark:
            self.spark.stop()
            self.spark = None
        
        # X√≥a th∆∞ m·ª•c t·∫°m n·∫øu c√≥
        if self.temp_model_dir and os.path.exists(self.temp_model_dir):
            try:
                shutil.rmtree(self.temp_model_dir, ignore_errors=True)
                print(f"‚úì ƒê√£ x√≥a th∆∞ m·ª•c t·∫°m: {self.temp_model_dir}")
            except Exception:
                pass

# Global predictor instance
_predictor = None

def get_predictor():
    """L·∫•y singleton instance c·ªßa predictor"""
    global _predictor
    if _predictor is None:
        # C√≥ th·ªÉ override model path b·∫±ng environment variable
        model_path = os.getenv("MODEL_PATH")
        if not model_path:
            # Th·ª≠ HDFS path tr∆∞·ªõc, fallback v·ªÅ local
            hdfs_namenode = os.getenv("HDFS_NAMENODE", "hdfs://worker1:8020")
            hdfs_model_path = os.getenv("HDFS_MODEL_PATH", f"{hdfs_namenode}/bigdata/model")
            # Th·ª≠ load t·ª´ HDFS, n·∫øu kh√¥ng ƒë∆∞·ª£c th√¨ fallback v·ªÅ local
            try:
                # Test xem c√≥ th·ªÉ access HDFS kh√¥ng
                cmd = ['hdfs', 'dfs', '-test', '-e', hdfs_model_path]
                result = subprocess.run(cmd, capture_output=True, timeout=10)
                if result.returncode == 0:
                    model_path = hdfs_model_path
                    print(f"üìÇ S·ª≠ d·ª•ng model t·ª´ HDFS: {hdfs_model_path}")
                else:
                    model_path = "models/house_price_model"
                    print(f"üìÇ S·ª≠ d·ª•ng model local: {model_path}")
            except Exception:
                model_path = "models/house_price_model"
                print(f"üìÇ S·ª≠ d·ª•ng model local (fallback): {model_path}")
        
        _predictor = HousePricePredictor(model_path=model_path)
    return _predictor

