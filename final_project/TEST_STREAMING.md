# HÆ°á»›ng Dáº«n Test DAG 02 - Streaming Pipeline

## Kiáº¿n TrÃºc
```
Producer (Nole3) â†’ Kafka (Nole2) â†’ Spark Streaming (Nole1) â†’ Model (HDFS/Nole3) â†’ Kafka (Nole2) â†’ Consumer (Nole3)
```

## BÆ°á»›c 1: Chuáº©n Bá»‹ (TrÃªn Nole3)

### 1.1. Kiá»ƒm tra Kafka Ä‘ang cháº¡y trÃªn Nole2
```bash
ssh nole2@192.168.80.51 "docker ps | grep kafka"
```

Náº¿u chÆ°a cháº¡y:
```bash
ssh nole2@192.168.80.51 "cd ~/kafka_docker && docker-compose up -d"
```

### 1.2. Táº¡o Kafka Topics
```bash
ssh nole2@192.168.80.51 "docker exec kafka kafka-topics --create --if-not-exists --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic house_input"

ssh nole2@192.168.80.51 "docker exec kafka kafka-topics --create --if-not-exists --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic house_prediction"
```

### 1.3. Verify Topics
```bash
ssh nole2@192.168.80.51 "docker exec kafka kafka-topics --list --bootstrap-server localhost:9092"
```

Pháº£i tháº¥y:
- house_input
- house_prediction

### 1.4. Kiá»ƒm tra Model Ä‘Ã£ train
```bash
~/hadoop/bin/hdfs dfs -ls /bigdata/house_prices/model/
```

Náº¿u chÆ°a cÃ³ â†’ Cháº¡y DAG 01 trÆ°á»›c!

## BÆ°á»›c 2: Start Spark Streaming Job (TrÃªn Nole1)

SSH vÃ o Nole1:
```bash
ssh nole1@192.168.80.165
```

Submit Spark Streaming job:
```bash
~/spark/bin/spark-submit \
  --master spark://192.168.80.165:7077 \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  --conf spark.hadoop.fs.defaultFS=hdfs://192.168.80.178:8020 \
  --conf spark.driver.memory=2g \
  --conf spark.executor.memory=2g \
  ~/Tuan/Project/BigData/final_project/spark_code/spark_streaming.py
```

**LÆ°u Ã½:** Job nÃ y cháº¡y liÃªn tá»¥c, giá»¯ terminal nÃ y má»Ÿ!

## BÆ°á»›c 3: Start Consumer (TrÃªn Nole3 - Terminal 1)

```bash
cd ~/Tuan/Project/BigData/final_project
python3 kafka_consumer.py
```

Consumer sáº½ Ä‘á»£i predictions tá»« Kafka.

## BÆ°á»›c 4: Start Producer (TrÃªn Nole3 - Terminal 2)

```bash
cd ~/Tuan/Project/BigData/final_project
python3 kafka_producer.py
```

Producer sáº½:
- Äá»c `data/stream_data.csv`
- Gá»­i house features vÃ o Kafka
- Má»—i 2 giÃ¢y gá»­i 1 message

## Káº¿t Quáº£ Mong Äá»£i

**Terminal 1 (Consumer):**
```
[Prediction #1]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“Š Input Features:
   square_footage: 4012.0
   num_bedrooms: 3.0
   num_bathrooms: 12.0
   year_built: 2016.0
   ...

ğŸ’° Predicted Price: $901,000.49

â° Timestamp: 2025-12-22 12:00:00
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Terminal 2 (Producer):**
```
[1] Sent house data:
    Square Footage: 4012.0
    Bedrooms: 3.0
    Bathrooms: 12.0
    Year Built: 2016.0
    â†’ Partition: 0, Offset: 0
```

## Troubleshooting

### Producer lá»—i "KafkaTimeoutError"
```bash
# Check Kafka connectivity
telnet 192.168.80.51 9092

# Check topics exist
ssh nole2@192.168.80.51 "docker exec kafka kafka-topics --list --bootstrap-server localhost:9092"
```

### Spark Streaming khÃ´ng nháº­n data
```bash
# Check Spark UI
# http://192.168.80.165:4040

# Check Kafka cÃ³ data
ssh nole2@192.168.80.51 "docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic house_input --from-beginning --max-messages 1"
```

### Consumer khÃ´ng nháº­n predictions
```bash
# Check output topic cÃ³ data
ssh nole2@192.168.80.51 "docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic house_prediction --from-beginning --max-messages 1"
```

## Hoáº·c DÃ¹ng Airflow (Tá»± Äá»™ng)

1. Má»Ÿ Airflow UI: http://192.168.80.178:8080
2. Enable DAG: `02_Realtime_Streaming_Service`
3. Trigger DAG â–¶ï¸
4. Äá»£i Spark Streaming job start
5. Cháº¡y Producer vÃ  Consumer nhÆ° bÆ°á»›c 3-4

---

**Luá»“ng hoÃ n chá»‰nh:**
```
Producer â†’ house_input (Kafka) â†’ Spark Streaming â†’ Load Model (HDFS) â†’ Predict â†’ house_prediction (Kafka) â†’ Consumer
```
