# ML Streaming Pipeline vá»›i Spark, Kafka vÃ  Airflow

Dá»± Ã¡n nÃ y triá»ƒn khai má»™t pipeline há»c mÃ¡y end-to-end sá»­ dá»¥ng Spark ML, Kafka vÃ  Airflow Ä‘á»ƒ:
- Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n giÃ¡ nhÃ 
- Streaming dá»¯ liá»‡u qua Kafka
- Dá»± Ä‘oÃ¡n real-time vá»›i Spark Streaming
- Trá»±c quan hÃ³a káº¿t quáº£

## ğŸš€ Quick Start

**Há»‡ thá»‘ng phÃ¢n tÃ¡n vá»›i Hadoop HDFS vÃ  RabbitMQ (khÃ´ng dÃ¹ng SSH, chá»‰ dÃ¹ng hostname)**

ğŸ‘‰ **Xem [QUICK_START.md](QUICK_START.md) Ä‘á»ƒ báº¯t Ä‘áº§u nhanh**

Hoáº·c xem hÆ°á»›ng dáº«n chi tiáº¿t:
- **[SETUP_GUIDE.md](SETUP_GUIDE.md)**: HÆ°á»›ng dáº«n setup tá»«ng bÆ°á»›c vá»›i hostname
- **[README_HADOOP_RABBITMQ.md](README_HADOOP_RABBITMQ.md)**: Tá»•ng quan há»‡ thá»‘ng phÃ¢n tÃ¡n
- **[RABBITMQ_CONFIG.md](RABBITMQ_CONFIG.md)**: Cáº¥u hÃ¬nh RabbitMQ chi tiáº¿t

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- Python 3.9+
- Spark 4.0.0
- Kafka 3.8.0 (cháº¡y qua Docker)
- Docker vÃ  Docker Compose
- Airflow (Ä‘á»ƒ Ä‘iá»u khiá»ƒn pipeline)

## ğŸš€ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t Python dependencies

```bash
pip install -r requirements.txt
```

### 2. CÃ i Ä‘áº·t Spark 4.0.0

Táº£i vÃ  cÃ i Ä‘áº·t Spark 4.0.0 tá»« [Apache Spark Downloads](https://spark.apache.org/downloads.html)

```bash
# VÃ­ dá»¥ trÃªn Linux
wget https://archive.apache.org/dist/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz
tar -xzf spark-4.0.0-bin-hadoop3.tgz
export SPARK_HOME=/path/to/spark-4.0.0-bin-hadoop3
export PATH=$PATH:$SPARK_HOME/bin
```

### 3. CÃ i Ä‘áº·t Airflow

```bash
# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t Airflow
pip install apache-airflow==2.7.0
pip install apache-airflow-providers-apache-spark==4.0.0

# Khá»Ÿi táº¡o Airflow database
airflow db init

# Táº¡o user admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
final_project/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ml_pipeline_dag.py          # Airflow DAG Ä‘iá»u khiá»ƒn toÃ n bá»™
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prepare_data.py              # Chia dá»¯ liá»‡u train/streaming
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # Kafka + Zookeeper
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ train_model.py               # Huáº¥n luyá»‡n mÃ´ hÃ¬nh Spark ML
â”‚   â””â”€â”€ streaming_predict.py        # Spark Streaming dá»± Ä‘oÃ¡n
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ kafka_producer.py            # MÃ´ phá»ng streaming vÃ o Kafka
â”œâ”€â”€ visualization/
â”‚   â””â”€â”€ kafka_consumer.py            # Trá»±c quan hÃ³a káº¿t quáº£
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # File nÃ y
```

## ğŸ¯ CÃ¡ch cháº¡y

### PhÆ°Æ¡ng phÃ¡p 1: Cháº¡y qua Airflow (Khuyáº¿n nghá»‹)

#### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Airflow

```bash
# Terminal 1: Khá»Ÿi Ä‘á»™ng Airflow webserver
airflow webserver --port 8080

# Terminal 2: Khá»Ÿi Ä‘á»™ng Airflow scheduler
airflow scheduler
```

Truy cáº­p Airflow UI: http://localhost:8080
- Username: admin
- Password: admin

#### BÆ°á»›c 2: Cáº¥u hÃ¬nh Spark connection trong Airflow

1. VÃ o Airflow UI â†’ Admin â†’ Connections
2. TÃ¬m hoáº·c táº¡o connection vá»›i ID: `spark_default`
3. Cáº¥u hÃ¬nh:
   - Conn Type: `Spark`
   - Host: `local[*]` (hoáº·c Spark master URL cá»§a báº¡n)
   - Port: `7077` (náº¿u dÃ¹ng Spark standalone)
   - Extra: `{"queue": "default"}`

#### BÆ°á»›c 3: Cháº¡y DAG

1. VÃ o Airflow UI â†’ DAGs
2. TÃ¬m DAG `ml_streaming_pipeline`
3. Báº­t DAG (toggle switch)
4. Click "Trigger DAG" Ä‘á»ƒ cháº¡y

DAG sáº½ tá»± Ä‘á»™ng:
- âœ… Khá»Ÿi Ä‘á»™ng Kafka (Docker)
- âœ… Kiá»ƒm tra Kafka sáºµn sÃ ng
- âœ… Chuáº©n bá»‹ dá»¯ liá»‡u
- âœ… Huáº¥n luyá»‡n mÃ´ hÃ¬nh Spark ML
- âœ… Khá»Ÿi Ä‘á»™ng Spark Streaming job
- âœ… Gá»­i dá»¯ liá»‡u streaming vÃ o Kafka
- âœ… Äá»£i xá»­ lÃ½ hoÃ n thÃ nh
- âœ… Dá»n dáº¹p

#### BÆ°á»›c 4: Cháº¡y Visualization (tÃ¹y chá»n)

1. TÃ¬m DAG `ml_streaming_visualization`
2. Trigger DAG Ä‘á»ƒ cháº¡y consumer vÃ  hiá»ƒn thá»‹ biá»ƒu Ä‘á»“

---

### PhÆ°Æ¡ng phÃ¡p 2: Cháº¡y thá»§ cÃ´ng tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Kafka

```bash
cd docker
docker-compose up -d
```

Kiá»ƒm tra Kafka Ä‘Ã£ cháº¡y:
```bash
docker ps
```

#### BÆ°á»›c 2: Chuáº©n bá»‹ dá»¯ liá»‡u

```bash
cd /home/haminhchien/Documents/bigdata/final_project
python data/prepare_data.py
```

Káº¿t quáº£:
- `data/train_data.csv` - Dá»¯ liá»‡u huáº¥n luyá»‡n
- `data/streaming_data.csv` - Dá»¯ liá»‡u streaming

#### BÆ°á»›c 3: Huáº¥n luyá»‡n mÃ´ hÃ¬nh

```bash
spark-submit \
    --master local[*] \
    --driver-memory 4g \
    --executor-memory 4g \
    spark_jobs/train_model.py
```

MÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c lÆ°u vÃ o: `models/house_price_model/`

#### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Spark Streaming job

```bash
spark-submit \
    --master local[*] \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 \
    --driver-memory 4g \
    --executor-memory 4g \
    spark_jobs/streaming_predict.py
```

Job nÃ y sáº½ cháº¡y liÃªn tá»¥c, Ä‘á»c tá»« Kafka topic `house-prices-input` vÃ  gá»­i káº¿t quáº£ vÃ o `house-prices-output`.

#### BÆ°á»›c 5: Gá»­i dá»¯ liá»‡u streaming (Terminal má»›i)

```bash
python streaming/kafka_producer.py 1 200
```

Tham sá»‘:
- `1`: Khoáº£ng thá»i gian giá»¯a cÃ¡c message (giÃ¢y)
- `200`: Sá»‘ lÆ°á»£ng records gá»­i (None = táº¥t cáº£)

#### BÆ°á»›c 6: Trá»±c quan hÃ³a káº¿t quáº£ (Terminal má»›i)

```bash
python visualization/kafka_consumer.py
```

Sáº½ hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ real-time so sÃ¡nh giÃ¡ thá»±c táº¿ vs dá»± Ä‘oÃ¡n.

---

## ğŸ”§ Cáº¥u hÃ¬nh

### Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n project trong DAG

Náº¿u project path khÃ¡c, sá»­a trong `dags/ml_pipeline_dag.py`:

```python
params={'project_dir': '/your/project/path'}
```

### Thay Ä‘á»•i cáº¥u hÃ¬nh Spark

Sá»­a memory vÃ  cores trong:
- `dags/ml_pipeline_dag.py` (task `train_model` vÃ  `start_streaming_job`)
- `spark_jobs/train_model.py`
- `spark_jobs/streaming_predict.py`

### Thay Ä‘á»•i Kafka settings

Sá»­a trong `docker/docker-compose.yml`:
- Ports
- Memory limits
- Topic replication factor

---

## ğŸ› Troubleshooting

### Lá»—i: Kafka khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c

```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
docker ps | grep kafka

# Xem logs
docker logs kafka

# Restart Kafka
cd docker
docker-compose restart
```

### Lá»—i: Spark khÃ´ng tÃ¬m tháº¥y package

Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘Ãºng version:
```bash
pip install pyspark==4.0.0
```

VÃ  package spark-sql-kafka Ä‘Ãºng version:
```
org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0
```

### Lá»—i: Airflow khÃ´ng submit Ä‘Æ°á»£c Spark job

1. Kiá»ƒm tra Spark connection trong Airflow UI
2. Äáº£m báº£o `SPARK_HOME` Ä‘Æ°á»£c set trong environment
3. Kiá»ƒm tra Airflow cÃ³ quyá»n truy cáº­p Spark

### Lá»—i: Model khÃ´ng tÃ¬m tháº¥y

Äáº£m báº£o Ä‘Ã£ cháº¡y `train_model.py` trÆ°á»›c khi cháº¡y streaming:
```bash
ls models/house_price_model/
```

---

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

### Sau khi huáº¥n luyá»‡n:
- MÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u trong `models/house_price_model/`
- Metrics: RMSE, MAE, RÂ² Ä‘Æ°á»£c in ra console

### Sau khi streaming:
- Dá»¯ liá»‡u Ä‘Æ°á»£c gá»­i vÃ o Kafka topic `house-prices-input`
- Spark xá»­ lÃ½ vÃ  gá»­i káº¿t quáº£ vÃ o `house-prices-output`
- Biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ so sÃ¡nh actual vs predicted prices

---

## ğŸ“ Notes

- Spark 4.0.0 yÃªu cáº§u Scala 2.13
- Kafka 3.8.0 tÆ°Æ¡ng thÃ­ch vá»›i Spark 4.0.0
- Táº¥t cáº£ dependencies pháº£i tÆ°Æ¡ng thÃ­ch vá»›i Scala 2.13

---

## ğŸ‘¤ TÃ¡c giáº£

Final Project - Big Data

# big_data-final
