# ML Streaming Pipeline vá»›i Spark, Kafka vÃ  Airflow

Dá»± Ã¡n nÃ y triá»ƒn khai má»™t pipeline há»c mÃ¡y end-to-end sá»­ dá»¥ng Spark ML, Kafka vÃ  Airflow Ä‘á»ƒ:
- Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n giÃ¡ nhÃ 
- Streaming dá»¯ liá»‡u qua Kafka
- Dá»± Ä‘oÃ¡n real-time vá»›i Spark Streaming
- Trá»±c quan hÃ³a káº¿t quáº£

## ğŸš€ Quick Start

**Há»‡ thá»‘ng phÃ¢n tÃ¡n vá»›i Hadoop HDFS vÃ  RabbitMQ (khÃ´ng dÃ¹ng SSH, chá»‰ dÃ¹ng hostname)**

ğŸ‘‰ **Xem [QUICK_START.md](QUICK_START.md) Ä‘á»ƒ báº¯t Ä‘áº§u nhanh**

Hoáº·c xem hÆ°á»›ng dáº«n chi tiáº¿t:
- **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)**: Kiáº¿n trÃºc há»‡ thá»‘ng phÃ¢n tÃ¡n
- **[docs/BAO_CAO_CHI_TIET.md](docs/BAO_CAO_CHI_TIET.md)**: BÃ¡o cÃ¡o chi tiáº¿t dá»± Ã¡n
- **[docs/TEST_STREAMING.md](docs/TEST_STREAMING.md)**: HÆ°á»›ng dáº«n test streaming
- **[docs/archived/](docs/archived/)**: TÃ i liá»‡u lÆ°u trá»¯ (setup guides, troubleshooting)

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

- Python 3.9+
- Spark 4.0.0
- Kafka 3.8.0 (cháº¡y qua Docker)
- Docker vÃ  Docker Compose
- Airflow (Ä‘á»ƒ Ä‘iá»u khiá»ƒn pipeline)

## ğŸš€ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t Python dependencies

```bash
pip install -r requirements.txt
```

### 2. CÃ i Ä‘áº·t Spark 4.0.0

Táº£i vÃ  cÃ i Ä‘áº·t Spark 4.0.0 tá»« [Apache Spark Downloads](https://spark.apache.org/downloads.html)

```bash
# VÃ­ dá»¥ trÃªn Linux
wget https://archive.apache.org/dist/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz
tar -xzf spark-4.0.0-bin-hadoop3.tgz
export SPARK_HOME=/path/to/spark-4.0.0-bin-hadoop3
export PATH=$PATH:$SPARK_HOME/bin
```

### 3. CÃ i Ä‘áº·t Airflow

```bash
# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t Airflow
pip install apache-airflow==2.7.0
pip install apache-airflow-providers-apache-spark==4.0.0

# Khá»Ÿi táº¡o Airflow database
airflow db init

# Táº¡o user admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
final_project/
â”œâ”€â”€ README.md                          # File nÃ y - tá»•ng quan dá»± Ã¡n
â”œâ”€â”€ QUICK_START.md                     # HÆ°á»›ng dáº«n báº¯t Ä‘áº§u nhanh
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .env.example                       # Template cho environment variables
â”œâ”€â”€ .gitignore                         # Git ignore patterns
â”‚
â”œâ”€â”€ docs/                              # ğŸ“š Táº¥t cáº£ documentation
â”‚   â”œâ”€â”€ README.md                      # Tá»•ng quan documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md                # Kiáº¿n trÃºc há»‡ thá»‘ng
â”‚   â”œâ”€â”€ BAO_CAO_CHI_TIET.md           # BÃ¡o cÃ¡o chi tiáº¿t
â”‚   â”œâ”€â”€ TEST_STREAMING.md             # HÆ°á»›ng dáº«n test
â”‚   â””â”€â”€ archived/                     # Docs cÅ© (lÆ°u trá»¯)
â”‚
â”œâ”€â”€ dags/                              # âœˆï¸ Airflow DAGs
â”‚   â”œâ”€â”€ 01_Infrastructure_and_Training_Pipeline.py
â”‚   â””â”€â”€ 02_Realtime_Streaming_Service.py
â”‚
â”œâ”€â”€ spark_code/                        # âš¡ Spark jobs
â”‚   â”œâ”€â”€ train_model.py                # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
â”‚   â””â”€â”€ spark_streaming.py            # Streaming prediction
â”‚
â”œâ”€â”€ streaming/                         # ğŸ“¤ Kafka producer
â”‚   â””â”€â”€ kafka_producer.py             # Äá»c tá»« HDFS, gá»­i vÃ o Kafka
â”‚
â”œâ”€â”€ visualization/                     # ğŸ“Š Kafka consumer & visualization
â”‚   â””â”€â”€ kafka_consumer.py             # Real-time visualization
â”‚
â”œâ”€â”€ web/                               # ğŸŒ Web UI
â”‚   â”œâ”€â”€ app.py                        # Flask application
â”‚   â”œâ”€â”€ predict_service.py            # Prediction service
â”‚   â”œâ”€â”€ dashboard.html                # Dashboard template
â”‚   â””â”€â”€ requirements.txt              # Web UI dependencies
â”‚
â”œâ”€â”€ data/                              # ğŸ“Š Data preparation
â”‚   â”œâ”€â”€ prepare_data.py               # Chia dá»¯ liá»‡u train/streaming
â”‚   â””â”€â”€ upload_to_hdfs.py             # Upload lÃªn HDFS
â”‚
â”œâ”€â”€ mycelery/                          # ğŸ”§ Celery workers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ system_worker.py              # Worker tasks
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ scripts/                           # ğŸ”§ Shell scripts (organized)
â”‚   â”œâ”€â”€ README.md                     # Scripts documentation
â”‚   â”œâ”€â”€ setup/                        # Setup scripts
â”‚   â”œâ”€â”€ workers/                      # Worker management
â”‚   â”œâ”€â”€ checks/                       # Health checks
â”‚   â”œâ”€â”€ fixes/                        # Fix scripts
â”‚   â””â”€â”€ utils/                        # Utilities
â”‚
â”œâ”€â”€ config/                            # âš™ï¸ Configuration files
â”‚   â”œâ”€â”€ airflow.cfg
â”‚   â””â”€â”€ kafka_docker_compose_fixed.yml
â”‚
â”œâ”€â”€ utils/                             # ğŸ› ï¸ Utility modules
â”‚
â””â”€â”€ models/                            # ğŸ’¾ Local model backup (optional)
```

## ğŸ¯ CÃ¡ch cháº¡y

### PhÆ°Æ¡ng phÃ¡p 1: Cháº¡y qua Airflow (Khuyáº¿n nghá»‹)

#### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Airflow

```bash
# Terminal 1: Khá»Ÿi Ä‘á»™ng Airflow webserver
airflow webserver --port 8080

# Terminal 2: Khá»Ÿi Ä‘á»™ng Airflow scheduler
airflow scheduler
```

Truy cáº­p Airflow UI: http://localhost:8080
- Username: admin
- Password: admin

#### BÆ°á»›c 2: Cáº¥u hÃ¬nh Spark connection trong Airflow

1. VÃ o Airflow UI â†’ Admin â†’ Connections
2. TÃ¬m hoáº·c táº¡o connection vá»›i ID: `spark_default`
3. Cáº¥u hÃ¬nh:
   - Conn Type: `Spark`
   - Host: `local[*]` (hoáº·c Spark master URL cá»§a báº¡n)
   - Port: `7077` (náº¿u dÃ¹ng Spark standalone)
   - Extra: `{"queue": "default"}`

#### BÆ°á»›c 3: Cháº¡y DAG

1. VÃ o Airflow UI â†’ DAGs
2. TÃ¬m DAG `ml_streaming_pipeline`
3. Báº­t DAG (toggle switch)
4. Click "Trigger DAG" Ä‘á»ƒ cháº¡y

DAG sáº½ tá»± Ä‘á»™ng:
- âœ… Khá»Ÿi Ä‘á»™ng Kafka (Docker)
- âœ… Kiá»ƒm tra Kafka sáºµn sÃ ng
- âœ… Chuáº©n bá»‹ dá»¯ liá»‡u
- âœ… Huáº¥n luyá»‡n mÃ´ hÃ¬nh Spark ML
- âœ… Khá»Ÿi Ä‘á»™ng Spark Streaming job
- âœ… Gá»­i dá»¯ liá»‡u streaming vÃ o Kafka
- âœ… Äá»£i xá»­ lÃ½ hoÃ n thÃ nh
- âœ… Dá»n dáº¹p

#### BÆ°á»›c 4: Cháº¡y Visualization (tÃ¹y chá»n)

1. TÃ¬m DAG `ml_streaming_visualization`
2. Trigger DAG Ä‘á»ƒ cháº¡y consumer vÃ  hiá»ƒn thá»‹ biá»ƒu Ä‘á»“

---

### PhÆ°Æ¡ng phÃ¡p 2: Cháº¡y thá»§ cÃ´ng tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Kafka

```bash
cd docker
docker-compose up -d
```

Kiá»ƒm tra Kafka Ä‘Ã£ cháº¡y:
```bash
docker ps
```

#### BÆ°á»›c 2: Chuáº©n bá»‹ dá»¯ liá»‡u

```bash
cd /home/haminhchien/Documents/bigdata/final_project
python data/prepare_data.py
```

Káº¿t quáº£:
- `data/train_data.csv` - Dá»¯ liá»‡u huáº¥n luyá»‡n
- `data/streaming_data.csv` - Dá»¯ liá»‡u streaming

#### BÆ°á»›c 3: Huáº¥n luyá»‡n mÃ´ hÃ¬nh

```bash
spark-submit \
    --master local[*] \
    --driver-memory 4g \
    --executor-memory 4g \
    spark_jobs/train_model.py
```

MÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c lÆ°u vÃ o: `models/house_price_model/`

#### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Spark Streaming job

```bash
spark-submit \
    --master local[*] \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0 \
    --driver-memory 4g \
    --executor-memory 4g \
    spark_jobs/streaming_predict.py
```

Job nÃ y sáº½ cháº¡y liÃªn tá»¥c, Ä‘á»c tá»« Kafka topic `house-prices-input` vÃ  gá»­i káº¿t quáº£ vÃ o `house-prices-output`.

#### BÆ°á»›c 5: Gá»­i dá»¯ liá»‡u streaming (Terminal má»›i)

```bash
python streaming/kafka_producer.py 1 200
```

Tham sá»‘:
- `1`: Khoáº£ng thá»i gian giá»¯a cÃ¡c message (giÃ¢y)
- `200`: Sá»‘ lÆ°á»£ng records gá»­i (None = táº¥t cáº£)

#### BÆ°á»›c 6: Trá»±c quan hÃ³a káº¿t quáº£ (Terminal má»›i)

```bash
python visualization/kafka_consumer.py
```

Sáº½ hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ real-time so sÃ¡nh giÃ¡ thá»±c táº¿ vs dá»± Ä‘oÃ¡n.

---

## ğŸ”§ Cáº¥u hÃ¬nh

### Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n project trong DAG

Náº¿u project path khÃ¡c, sá»­a trong `dags/ml_pipeline_dag.py`:

```python
params={'project_dir': '/your/project/path'}
```

### Thay Ä‘á»•i cáº¥u hÃ¬nh Spark

Sá»­a memory vÃ  cores trong:
- `dags/ml_pipeline_dag.py` (task `train_model` vÃ  `start_streaming_job`)
- `spark_jobs/train_model.py`
- `spark_jobs/streaming_predict.py`

### Thay Ä‘á»•i Kafka settings

Sá»­a trong `docker/docker-compose.yml`:
- Ports
- Memory limits
- Topic replication factor

---

## ğŸ› Troubleshooting

### Lá»—i: Kafka khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c

```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
docker ps | grep kafka

# Xem logs
docker logs kafka

# Restart Kafka
cd docker
docker-compose restart
```

### Lá»—i: Spark khÃ´ng tÃ¬m tháº¥y package

Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘Ãºng version:
```bash
pip install pyspark==4.0.0
```

VÃ  package spark-sql-kafka Ä‘Ãºng version:
```
org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0
```

### Lá»—i: Airflow khÃ´ng submit Ä‘Æ°á»£c Spark job

1. Kiá»ƒm tra Spark connection trong Airflow UI
2. Äáº£m báº£o `SPARK_HOME` Ä‘Æ°á»£c set trong environment
3. Kiá»ƒm tra Airflow cÃ³ quyá»n truy cáº­p Spark

### Lá»—i: Model khÃ´ng tÃ¬m tháº¥y

Äáº£m báº£o Ä‘Ã£ cháº¡y `train_model.py` trÆ°á»›c khi cháº¡y streaming:
```bash
ls models/house_price_model/
```

---

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

### Sau khi huáº¥n luyá»‡n:
- MÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u trong `models/house_price_model/`
- Metrics: RMSE, MAE, RÂ² Ä‘Æ°á»£c in ra console

### Sau khi streaming:
- Dá»¯ liá»‡u Ä‘Æ°á»£c gá»­i vÃ o Kafka topic `house-prices-input`
- Spark xá»­ lÃ½ vÃ  gá»­i káº¿t quáº£ vÃ o `house-prices-output`
- Biá»ƒu Ä‘á»“ hiá»ƒn thá»‹ so sÃ¡nh actual vs predicted prices

---

## ğŸ“ Notes

- Spark 4.0.0 yÃªu cáº§u Scala 2.13
- Kafka 3.8.0 tÆ°Æ¡ng thÃ­ch vá»›i Spark 4.0.0
- Táº¥t cáº£ dependencies pháº£i tÆ°Æ¡ng thÃ­ch vá»›i Scala 2.13

---

## ğŸ‘¤ TÃ¡c giáº£

Final Project - Big Data

# big_data-final
