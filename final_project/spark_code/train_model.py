"""
Spark ML job ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS v√† l∆∞u model l√™n HDFS
"""
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
import os
import sys
import time

# C·∫•u h√¨nh HDFS v√† Spark - NEW ARCHITECTURE
HDFS_NAMENODE = "hdfs://192.168.80.178:8020"
HDFS_TRAIN_DATA_PATH = f"{HDFS_NAMENODE}/bigdata/house_prices/train_data.csv"
HDFS_MODEL_PATH = f"{HDFS_NAMENODE}/bigdata/house_prices/model"

def log(msg):
    """Log v·ªõi flush ngay ƒë·ªÉ xem progress"""
    print(msg, flush=True)
    sys.stdout.flush()

def train_model():
    log("=" * 60)
    log("B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH")
    log("=" * 60)
    log(f"HDFS Namenode: {HDFS_NAMENODE}")
    log(f"HDFS Train Data: {HDFS_TRAIN_DATA_PATH}")
    log(f"HDFS Model Path: {HDFS_MODEL_PATH}")
    
    # Create Spark session
    log("\n[STEP 1/7] Creating Spark session...")
    spark = SparkSession.builder \
        .appName("HousePriceModelTraining") \
        .config("spark.hadoop.fs.defaultFS", HDFS_NAMENODE) \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("ERROR")
    log(f"‚úì Spark Session created. App ID: {spark.sparkContext.applicationId}")
    
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
    log(f"\n[STEP 2/7] üìÇ ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS: {HDFS_TRAIN_DATA_PATH}")
    try:
        df = spark.read.csv(HDFS_TRAIN_DATA_PATH, header=True, inferSchema=True)
        log("‚úì ƒê√£ load DataFrame")
    except Exception as e:
        log(f"‚ùå L·ªói khi ƒë·ªçc d·ªØ li·ªáu: {e}")
        spark.stop()
        sys.exit(1)
    
    # B∆Ø·ªöC 1: Chu·∫©n h√≥a t√™n c·ªôt
    from pyspark.sql.functions import col as spark_col
    
    log("\nChu·∫©n h√≥a t√™n c·ªôt...")
    for c in df.columns:
        new_c = c.strip()
        new_c = new_c.replace(".", "_")
        new_c = new_c.replace(" ", "_")
        new_c = new_c.replace("-", "_")
        
        if new_c and new_c[0].isdigit():
            new_c = "f_" + new_c
        
        if new_c != c:
            df = df.withColumnRenamed(c, new_c)
    
    log("‚úì T√™n c·ªôt sau khi chu·∫©n h√≥a:")
    log(str(df.columns))
    
    # B∆Ø·ªöC 2: X√°c ƒë·ªãnh c·ªôt label (price)
    if "price" in df.columns:
        label_col = "price"
        log("‚úì T√¨m th·∫•y c·ªôt 'price'")
    else:
        # CSV kh√¥ng c√≥ header ƒë√∫ng, d√πng c·ªôt cu·ªëi l√†m target
        label_col = df.columns[-1]
        log(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·ªôt 'price', d√πng c·ªôt cu·ªëi l√†m target: {label_col}")
        # Rename c·ªôt cu·ªëi th√†nh 'price' ƒë·ªÉ code ph√≠a sau d√πng
        df = df.withColumnRenamed(label_col, "price")
        label_col = "price"
        log("‚úì ƒê√£ rename c·ªôt target th√†nh 'price'")
    
    log(f"‚úì Label column: {label_col}")
    
    # ƒê·∫øm s·ªë m·∫´u
    log(f"\n[STEP 3/7] ƒê·∫øm s·ªë m·∫´u...")
    try:
        num_samples = df.count()
        log(f"‚úì ƒê√£ ƒë·ªçc {num_samples} m·∫´u")
    except Exception as e:
        log(f"‚ùå L·ªói khi ƒë·∫øm: {e}")
        spark.stop()
        sys.exit(1)
    
    log("\nSchema:")
    df.printSchema()
    
    # B∆Ø·ªöC 3: Ch·ªâ l·∫•y feature l√† s·ªë (numeric types)
    from pyspark.sql.types import NumericType
    
    feature_cols = [
        f.name for f in df.schema.fields
        if f.name != "price" and isinstance(f.dataType, NumericType)
    ]
    
    log(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")
    
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features"
    )
    
    # Random Forest model
    rf = RandomForestRegressor(
        featuresCol="features",
        labelCol="price",
        numTrees=100,
        maxDepth=10,
        seed=42
    )
    
    # Pipeline
    pipeline = Pipeline(stages=[assembler, rf])
    
    # Chia train/test 80/20
    log(f"\n[STEP 4/7] Chia d·ªØ li·ªáu train/test (80/20)...")
    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)
    
    train_count = train_data.count()
    test_count = test_data.count()
    log(f"‚úì Train samples: {train_count}")
    log(f"‚úì Test samples: {test_count}")
    
    # Training
    log("\n[STEP 5/7] üîÑ Training Random Forest model...")
    log(f"   Trees: 100, Max depth: 10")
    log("   ‚è≥ B·∫Øt ƒë·∫ßu training (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    
    fit_start = time.time()
    try:
        model = pipeline.fit(train_data)
        fit_time = time.time() - fit_start
        log(f"‚úì Training ho√†n th√†nh! ({fit_time:.2f}s)")
    except Exception as e:
        log(f"‚ùå L·ªói khi training: {e}")
        spark.stop()
        sys.exit(1)
    
    # Evaluation
    log("\n[STEP 6/7] ƒê√°nh gi√° m√¥ h√¨nh...")
    predictions = model.transform(test_data)
    
    evaluator_rmse = RegressionEvaluator(
        labelCol="price",
        predictionCol="prediction",
        metricName="rmse"
    )
    
    evaluator_r2 = RegressionEvaluator(
        labelCol="price",
        predictionCol="prediction",
        metricName="r2"
    )
    
    evaluator_mae = RegressionEvaluator(
        labelCol="price",
        predictionCol="prediction",
        metricName="mae"
    )
    
    rmse = evaluator_rmse.evaluate(predictions)
    r2 = evaluator_r2.evaluate(predictions)
    mae = evaluator_mae.evaluate(predictions)
    
    log("\n" + "=" * 60)
    log("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH")
    log("=" * 60)
    log(f"RMSE: {rmse:.4f}")
    log(f"MAE:  {mae:.4f}")
    log(f"R¬≤:   {r2:.4f}")
    log("=" * 60)
    
    # L∆∞u model l√™n HDFS
    log(f"\n[STEP 7/7] üíæ L∆∞u model l√™n HDFS: {HDFS_MODEL_PATH}")
    
    try:
        save_start = time.time()
        model.write().overwrite().save(HDFS_MODEL_PATH)
        save_time = time.time() - save_start
        log(f"‚úì ƒê√£ l∆∞u model v√†o HDFS ({save_time:.2f}s)")
    except Exception as e:
        log(f"‚ùå L·ªói khi l∆∞u model: {e}")
        spark.stop()
        sys.exit(1)
    
    # Hi·ªÉn th·ªã m·ªôt s·ªë predictions
    log("\nM·ªôt s·ªë d·ª± ƒëo√°n m·∫´u:")
    predictions.select("price", "prediction").show(10, truncate=False)
    
    log("\n‚úì D·ª´ng Spark Session...")
    spark.stop()
    
    log("\n" + "=" * 60)
    log("‚úì HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN!")
    log("=" * 60)

if __name__ == "__main__":
    train_model()
